{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58759719-c4c9-45df-90c6-4335d8344707",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class Mha_GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, max_seq_length, num_heads):\n",
    "        super(Mha_GPT, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(embedding_size, max_seq_length)\n",
    "        \n",
    "        self.multihead_self_attention = MultiheadSelfAttention(embedding_size, num_heads)\n",
    "       \n",
    "        self.add_norm1 = nn.LayerNorm(embedding_size) \n",
    "        \n",
    "        self.feed_forward = FeedForward(embedding_size, 10)\n",
    "        \n",
    "        self.add_norm = nn.LayerNorm(embedding_size)\n",
    "        \n",
    "        self.fc = nn.Linear(embedding_size, vocab_size)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        \n",
    "        embeddings = self.embedding(input_ids)    \n",
    "        \n",
    "        embeddings = self.positional_encoding(embeddings)\n",
    "        \n",
    "        attention_output, heads_attention_weights = self.multihead_self_attention(embeddings)\n",
    "        \n",
    "        output1 = embeddings + self.add_norm1(attention_output)\n",
    "        \n",
    "        ff_output = self.feed_forward(output1)\n",
    "        \n",
    "        output = output1 + self.add_norm(ff_output)       \n",
    "        \n",
    "        logits = self.fc(output)\n",
    "        \n",
    "        return logits, heads_attention_weights\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, input_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        position = torch.arange(0, max_seq_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pos_enc = torch.zeros(1, max_seq_length, d_model)\n",
    "        pos_enc[:, :, 0::2] = torch.sin(position * div_term)\n",
    "        pos_enc[:, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pos_enc', pos_enc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_enc[:, :x.size(1)].detach()\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_size, num_heads):\n",
    "        super(MultiheadSelfAttention, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        split_size = embedding_size // num_heads\n",
    "\n",
    "         # Linear projections for queries, keys, and values for each head\n",
    "        self.linear_queries = nn.ModuleList([nn.Linear(split_size, split_size) for _ in range(num_heads)])\n",
    "        self.linear_keys = nn.ModuleList([nn.Linear(split_size, split_size) for _ in range(num_heads)])\n",
    "        self.linear_values = nn.ModuleList([nn.Linear(split_size, split_size) for _ in range(num_heads)])\n",
    "        # Final linear projection\n",
    "        self.linear_out = nn.Linear(split_size * num_heads, embedding_size)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Split the embeddings into 'num_heads' parts\n",
    "        split_size = self.embedding_size // self.num_heads\n",
    "\n",
    "      \n",
    "        split_embeddings = embeddings.view(embeddings.size(0), embeddings.size(1), self.num_heads, split_size)\n",
    "\n",
    "       \n",
    "        \n",
    "        # Linear projections for queries, keys, and values for each head\n",
    "        queries = [linear(split_embeddings[:, :, i, :]) for i, linear in enumerate(self.linear_queries)]\n",
    "        keys = [linear(split_embeddings[:, :, i, :]) for i, linear in enumerate(self.linear_keys)]\n",
    "        values = [linear(split_embeddings[:, :, i, :]) for i, linear in enumerate(self.linear_values)]\n",
    "\n",
    "       \n",
    "     \n",
    "        # Perform attention independently for each head\n",
    "        attention_output_lists = [self._scaled_dot_product_attention(q, k, v) for q, k, v in zip(queries, keys, values)]\n",
    "        attention_outputs = [inner_list[0] for inner_list in attention_output_lists]\n",
    "        heads_attention_weights = [inner_list[1] for inner_list in attention_output_lists]\n",
    "        \n",
    "        # Concatenate the results from all heads\n",
    "        concatenated_attention = torch.cat(attention_outputs, dim=-1)\n",
    "     \n",
    "        # Apply the final linear projection\n",
    "        output = self.linear_out(concatenated_attention)\n",
    "\n",
    "        return output, heads_attention_weights\n",
    "\n",
    "    def _scaled_dot_product_attention(self, query, key, value):\n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / (query.size(-1) ** 0.5)\n",
    "\n",
    "        # Apply mask\n",
    "        seq_length = attention_scores.size(-1)\n",
    "        mask = torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1).bool().to(attention_scores.device)\n",
    "        attention_scores = attention_scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "       \n",
    "\n",
    "        # Apply softmax\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "\n",
    "        # Compute weighted sum (Attention Scores x V)\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "\n",
    "        return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edbcf985-4caf-4b4e-bb89-b6266978ab95",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentencepiece'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentencepiece\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspm\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sp \u001b[38;5;241m=\u001b[39m spm\u001b[38;5;241m.\u001b[39mSentencePieceProcessor()\n\u001b[1;32m      3\u001b[0m sp\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtinystories_tokeniser.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentencepiece'"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"tinystories_tokeniser.model\")\n",
    "\n",
    "vocab = [sp.id_to_piece(i) for i in range(sp.get_piece_size())]\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 32\n",
    "max_seq_length = 12\n",
    "list_of_lists = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "tensor_2d = torch.tensor(list_of_lists)\n",
    "mha_model = Mha_GPT(vocab_size, embedding_size, max_seq_length, 4)\n",
    "probs, heads_attention_weights = mha_model(tensor_2d)\n",
    "print(heads_attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c0232ee-8717-4794-bf6b-1bc4678eaa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5804,  2.0289,  0.9384,  ...,  0.0202, -0.9566,  0.8582],\n",
      "         [ 0.2776,  1.8028, -0.3687,  ..., -0.1361, -2.1818,  0.6668],\n",
      "         [ 0.5065,  2.2130,  0.4301,  ...,  1.1961, -0.7159,  0.9888]],\n",
      "\n",
      "        [[-0.1150,  0.7286, -0.4281,  ...,  0.4932, -0.9897,  2.1935],\n",
      "         [-0.0316,  1.5358, -0.7105,  ...,  0.5412, -0.6257,  2.7560],\n",
      "         [ 0.1027, -0.1298, -0.2231,  ..., -0.6187, -0.5578,  2.9701]],\n",
      "\n",
      "        [[-1.5034,  2.5926,  1.6077,  ..., -0.5684,  0.4391,  0.5368],\n",
      "         [-0.9994,  2.1132,  0.0809,  ..., -0.9080, -0.6356,  1.1818],\n",
      "         [-0.6180,  2.5011,  0.0144,  ..., -1.1487,  1.3814,  0.1739]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915ad75-58ac-4f84-afd4-0a4f6d9f1ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
