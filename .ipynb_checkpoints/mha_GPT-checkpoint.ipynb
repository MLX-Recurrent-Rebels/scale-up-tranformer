{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58759719-c4c9-45df-90c6-4335d8344707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class Mha_GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, max_seq_length, num_heads):\n",
    "        super(Mha_GPT, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(embedding_size, max_seq_length)\n",
    "        \n",
    "        self.multihead_self_attention = MultiheadSelfAttention(embedding_size, num_heads)\n",
    "       \n",
    "        self.add_norm1 = nn.LayerNorm(embedding_size) \n",
    "        \n",
    "        self.feed_forward = FeedForward(embedding_size, 10)\n",
    "        \n",
    "        self.add_norm = nn.LayerNorm(embedding_size)\n",
    "        \n",
    "        self.fc = nn.Linear(embedding_size, vocab_size)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        \n",
    "        embeddings = self.embedding(input_ids)    \n",
    "        \n",
    "        embeddings = self.positional_encoding(embeddings)\n",
    "        \n",
    "        attention_output, heads_attention_weights = self.multihead_self_attention(embeddings)\n",
    "        \n",
    "        output1 = embeddings + self.add_norm1(attention_output)\n",
    "        \n",
    "        ff_output = self.feed_forward(output1)\n",
    "        \n",
    "        output = output1 + self.add_norm(ff_output)       \n",
    "        \n",
    "        logits = self.fc(output)\n",
    "        \n",
    "        return logits, heads_attention_weights\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, input_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        position = torch.arange(0, max_seq_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pos_enc = torch.zeros(1, max_seq_length, d_model)\n",
    "        pos_enc[:, :, 0::2] = torch.sin(position * div_term)\n",
    "        pos_enc[:, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pos_enc', pos_enc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_enc[:, :x.size(1)].detach()\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_size, num_heads):\n",
    "        super(MultiheadSelfAttention, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        split_size = embedding_size // num_heads\n",
    "\n",
    "         # Linear projections for queries, keys, and values for each head\n",
    "        self.linear_queries = nn.ModuleList([nn.Linear(split_size, split_size) for _ in range(num_heads)])\n",
    "        self.linear_keys = nn.ModuleList([nn.Linear(split_size, split_size) for _ in range(num_heads)])\n",
    "        self.linear_values = nn.ModuleList([nn.Linear(split_size, split_size) for _ in range(num_heads)])\n",
    "        # Final linear projection\n",
    "        self.linear_out = nn.Linear(split_size * num_heads, embedding_size)\n",
    "\n",
    "    def forward(self, embeddings, mask=None):\n",
    "        # Split the embeddings into 'num_heads' parts\n",
    "        split_size = self.embedding_size // self.num_heads\n",
    "\n",
    "      \n",
    "        split_embeddings = embeddings.view(embeddings.size(0), embeddings.size(1), self.num_heads, split_size)\n",
    "\n",
    "       \n",
    "        \n",
    "        # Linear projections for queries, keys, and values for each head\n",
    "        queries = [linear(split_embeddings[:, :, i, :]) for i, linear in enumerate(self.linear_queries)]\n",
    "        keys = [linear(split_embeddings[:, :, i, :]) for i, linear in enumerate(self.linear_keys)]\n",
    "        values = [linear(split_embeddings[:, :, i, :]) for i, linear in enumerate(self.linear_values)]\n",
    "\n",
    "      \n",
    "     \n",
    "        # Perform attention independently for each head\n",
    "        attention_output_lists = [self._scaled_dot_product_attention(q, k, v, mask) for q, k, v in zip(queries, keys, values)]\n",
    "        attention_outputs = [inner_list[0] for inner_list in attention_output_lists]\n",
    "        heads_attention_weights = [inner_list[1] for inner_list in attention_output_lists]\n",
    "        \n",
    "        # Concatenate the results from all heads\n",
    "        concatenated_attention = torch.cat(attention_outputs, dim=-1)\n",
    "     \n",
    "        # Apply the final linear projection\n",
    "        output = self.linear_out(concatenated_attention)\n",
    "\n",
    "        return output, heads_attention_weights\n",
    "\n",
    "    def _scaled_dot_product_attention(self, query, key, value, mask=None):\n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / (query.size(-1) ** 0.5)\n",
    "\n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Apply softmax\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "\n",
    "        # Compute weighted sum (Attention Scores x V)\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "\n",
    "        return output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edbcf985-4caf-4b4e-bb89-b6266978ab95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[0.3566, 0.2985, 0.3449],\n",
      "         [0.2803, 0.5300, 0.1896],\n",
      "         [0.2498, 0.4374, 0.3128]],\n",
      "\n",
      "        [[0.3985, 0.3823, 0.2192],\n",
      "         [0.1069, 0.2132, 0.6798],\n",
      "         [0.0799, 0.3115, 0.6086]],\n",
      "\n",
      "        [[0.4470, 0.1556, 0.3974],\n",
      "         [0.4303, 0.2464, 0.3233],\n",
      "         [0.5613, 0.0830, 0.3557]]], grad_fn=<SoftmaxBackward0>), tensor([[[0.3559, 0.3340, 0.3101],\n",
      "         [0.3260, 0.3428, 0.3311],\n",
      "         [0.2630, 0.4204, 0.3166]],\n",
      "\n",
      "        [[0.3393, 0.3654, 0.2953],\n",
      "         [0.5932, 0.1683, 0.2385],\n",
      "         [0.2273, 0.1289, 0.6438]],\n",
      "\n",
      "        [[0.4670, 0.1724, 0.3606],\n",
      "         [0.6996, 0.1114, 0.1890],\n",
      "         [0.5667, 0.1947, 0.2386]]], grad_fn=<SoftmaxBackward0>), tensor([[[0.3358, 0.3086, 0.3556],\n",
      "         [0.3163, 0.2746, 0.4090],\n",
      "         [0.4141, 0.4912, 0.0947]],\n",
      "\n",
      "        [[0.3384, 0.3789, 0.2827],\n",
      "         [0.3592, 0.4100, 0.2308],\n",
      "         [0.3461, 0.3889, 0.2650]],\n",
      "\n",
      "        [[0.2714, 0.2393, 0.4893],\n",
      "         [0.3415, 0.2610, 0.3974],\n",
      "         [0.3147, 0.3393, 0.3459]]], grad_fn=<SoftmaxBackward0>), tensor([[[0.3158, 0.3412, 0.3430],\n",
      "         [0.3357, 0.3455, 0.3188],\n",
      "         [0.2395, 0.3823, 0.3782]],\n",
      "\n",
      "        [[0.4575, 0.2934, 0.2491],\n",
      "         [0.3318, 0.3560, 0.3122],\n",
      "         [0.4121, 0.2880, 0.2999]],\n",
      "\n",
      "        [[0.3646, 0.3091, 0.3263],\n",
      "         [0.3575, 0.3181, 0.3244],\n",
      "         [0.3328, 0.2541, 0.4131]]], grad_fn=<SoftmaxBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"tinystories_tokeniser.model\")\n",
    "\n",
    "vocab = [sp.id_to_piece(i) for i in range(sp.get_piece_size())]\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 32\n",
    "max_seq_length = 12\n",
    "list_of_lists = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "tensor_2d = torch.tensor(list_of_lists)\n",
    "mha_model = Mha_GPT(vocab_size, embedding_size, max_seq_length, 4)\n",
    "probs, heads_attention_weights = mha_model(tensor_2d)\n",
    "print(heads_attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c0232ee-8717-4794-bf6b-1bc4678eaa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5804,  2.0289,  0.9384,  ...,  0.0202, -0.9566,  0.8582],\n",
      "         [ 0.2776,  1.8028, -0.3687,  ..., -0.1361, -2.1818,  0.6668],\n",
      "         [ 0.5065,  2.2130,  0.4301,  ...,  1.1961, -0.7159,  0.9888]],\n",
      "\n",
      "        [[-0.1150,  0.7286, -0.4281,  ...,  0.4932, -0.9897,  2.1935],\n",
      "         [-0.0316,  1.5358, -0.7105,  ...,  0.5412, -0.6257,  2.7560],\n",
      "         [ 0.1027, -0.1298, -0.2231,  ..., -0.6187, -0.5578,  2.9701]],\n",
      "\n",
      "        [[-1.5034,  2.5926,  1.6077,  ..., -0.5684,  0.4391,  0.5368],\n",
      "         [-0.9994,  2.1132,  0.0809,  ..., -0.9080, -0.6356,  1.1818],\n",
      "         [-0.6180,  2.5011,  0.0144,  ..., -1.1487,  1.3814,  0.1739]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915ad75-58ac-4f84-afd4-0a4f6d9f1ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
