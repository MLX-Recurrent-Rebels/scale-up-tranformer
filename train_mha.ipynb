{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e894833a-1d83-4ba0-92b6-48ba7f468ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b42ab9-e9ed-42a0-b2d7-8606fd095e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eba4b7b1-b6b4-4d35-8402-8b4b87ecf02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import sentencepiece as spm\n",
    "#import my_GPT\n",
    "from mha_GPT import Mha_GPT\n",
    "#import pickle\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "from dataset_bes import TinyDataset\n",
    "#from TinyDataset import collate_fn\n",
    " \n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "# install for optimiser\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler = GradScaler()\n",
    "# Load the SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"tinystories_tokeniser.model\")\n",
    "\n",
    "vocab = [sp.id_to_piece(i) for i in range(sp.get_piece_size())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19ddcf95-b7b8-4a41-9427-5999c8398e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pulsar/miniconda3/envs/sut/lib/python3.11/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n",
      "/home/pulsar/miniconda3/envs/sut/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "train_data = data[\"train\"]\n",
    "val_data = data[\"validation\"]\n",
    "ds = dataset_bes.TinyDataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "121e9635-6026-423b-8d13-d2dd525f8943",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "dl = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True, collate_fn=ds.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1472f28-8696-4578-89dc-0b8e1e1930b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 512\n",
    "max_seq_length = 1194\n",
    "num_heads = 4\n",
    "#batch_size = 64  # You can adjust this based on your specific requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bdacb84-a686-4073-bfa3-b3f6917afa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = Mha_GPT(vocab_size, embedding_size, max_seq_length, num_heads)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9519f34-5b25-4c06-87e6-8e4d1f852201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "beta1 = 0.9\n",
    "beta2 = 0.98\n",
    "epsilon = 1e-9\n",
    "learning_rate = 0.001  # Adjust this based on your experimentation\n",
    "\n",
    "# Create Adam optimizer\n",
    "optimizer = Adam(gpt_model.parameters(), lr=learning_rate, betas=(beta1, beta2), eps=epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72747fbb-44d4-4a5e-b0c0-e62ee6f84baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'checkpoints/'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df750043-d132-47cc-b6fc-6b4e8feab7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbirsenyildiz2018\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/pulsar/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wandb.login(key='0a9583c37867b372d3f0849f8385a8eb0d4c4ec2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e994ae3e-7344-4d8e-9e35-071fdee28036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:80vv1nki) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.013 MB uploaded\\r'), FloatProgress(value=0.19016690582282306, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc_train</td><td>▁▄▆▇▆▇▇▇▇▇▇▇▇███████████████████████████</td></tr><tr><td>loss_train</td><td>▆█▃▅▆▅▃▇▄▃▅▄▅▄▃▄▄▄▄▁▄▄▄▅▄▄▁▃▃▃▅▃▄▄▄▅▃▄▃▆</td></tr><tr><td>perp_train</td><td>▃█▁▃▄▃▁▆▂▂▃▂▂▂▁▂▂▂▂▁▂▂▂▂▂▂▁▁▁▂▂▁▂▂▂▃▁▂▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc_train</td><td>0.48439</td></tr><tr><td>loss_train</td><td>3.84494</td></tr><tr><td>perp_train</td><td>46.7557</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">misunderstood-forest-93</strong> at: <a href='https://wandb.ai/birsenyildiz2018/Birsen_GPT/runs/80vv1nki' target=\"_blank\">https://wandb.ai/birsenyildiz2018/Birsen_GPT/runs/80vv1nki</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231114_154315-80vv1nki/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:80vv1nki). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de75a9a89b614ceba41cc572fe0d4b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112273758691218, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/pulsar/scale-up-transformer/wandb/run-20231114_160629-pbyzowpo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/birsenyildiz2018/Birsen_GPT/runs/pbyzowpo' target=\"_blank\">clean-rain-94</a></strong> to <a href='https://wandb.ai/birsenyildiz2018/Birsen_GPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/birsenyildiz2018/Birsen_GPT' target=\"_blank\">https://wandb.ai/birsenyildiz2018/Birsen_GPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/birsenyildiz2018/Birsen_GPT/runs/pbyzowpo' target=\"_blank\">https://wandb.ai/birsenyildiz2018/Birsen_GPT/runs/pbyzowpo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/birsenyildiz2018/Birsen_GPT/runs/pbyzowpo?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fcbd0240f90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project ='Birsen_GPT', entity = 'birsenyildiz2018',     \n",
    "    config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"architecture\": \"Multi headed GPT\",\n",
    "    \"dataset\": \"Tiny Stories\",\n",
    "    \"epochs\": 10, \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6eb0272a-8d4a-4eaf-a445-6ae1b53ae11b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory checkpoints does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m checkpoint_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     49\u001b[0m          checkpoint_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m          torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch,\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: gpt_model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_train\u001b[39m\u001b[38;5;124m'\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc_train\u001b[39m\u001b[38;5;124m'\u001b[39m: acc,\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperp_train\u001b[39m\u001b[38;5;124m'\u001b[39m: math\u001b[38;5;241m.\u001b[39mexp(loss\u001b[38;5;241m.\u001b[39mitem()),\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: batch_size,\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memd_dim\u001b[39m\u001b[38;5;124m'\u001b[39m: embedding_size,\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_heads\u001b[39m\u001b[38;5;124m'\u001b[39m: num_heads\n\u001b[1;32m     60\u001b[0m       }, checkpoint_path)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc_train\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/sut/lib/python3.11/site-packages/torch/serialization.py:618\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    615\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    619\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sut/lib/python3.11/site-packages/torch/serialization.py:492\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m container(name_or_buffer)\n",
      "File \u001b[0;32m~/miniconda3/envs/sut/lib/python3.11/site-packages/torch/serialization.py:463\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory checkpoints does not exist."
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "milestones = [1000, 2000, 3000]\n",
    "\n",
    "num_epochs = 10  # Adjust as needed\n",
    "checkpoint_interval = 1\n",
    "for epoch in range(num_epochs):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    step = 1\n",
    "    for batch in dl:\n",
    "       \n",
    "        tokens = batch['input']\n",
    "        true_labels = batch['label']\n",
    "        optimizer.zero_grad()\n",
    "        output = gpt_model(batch['input'])\n",
    "       \n",
    "        if isinstance(output, tuple) and len(output) == 2:\n",
    "            logits, attention_weights = output\n",
    "        else:\n",
    "        # Handle the case where your model returns only logits\n",
    "             logits = output\n",
    "             attention_weights = None  # You might need to adjust this based on your model's behavior\n",
    "        \n",
    "        model_output = logits.view(-1, vocab_size)  # Reshape to [batch_size * seq_length, num_classes]\n",
    "        \n",
    "        true_labels = true_labels.view(-1)  # Reshape to [batch_size * seq_length]\n",
    "        loss = criterion(model_output, true_labels)\n",
    "        max_indices = torch.argmax(model_output, dim=1)\n",
    "        correct_predictions += ((max_indices - true_labels)==0).sum()\n",
    "        total_predictions += len(true_labels)\n",
    "        # correct_predictions += torch.sum(predictions==label)\n",
    "        # total_predictions += batch_size\n",
    "        acc = correct_predictions/total_predictions\n",
    "        # acc = 0\n",
    "        with autocast():\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "\n",
    "        if step in milestones:\n",
    "        # Save a checkpoint\n",
    "           checkpoint_path = f'model_checkpoint_step_{step}.pth'\n",
    "           torch.save(gpt_model.state_dict(), checkpoint_path)\n",
    "\n",
    "        step += 1\n",
    "    # Logging and checkpointing\n",
    "        wandb.log({\"acc_train\": acc, \"loss_train\": loss.item(), \"perp_train\": math.exp(loss.item())})\n",
    "        \n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "             checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch + 1}.pt')\n",
    "             torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': gpt_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss_train': loss.item(),\n",
    "            'acc_train': acc,\n",
    "            'perp_train': math.exp(loss.item()),\n",
    "            'batch_size': batch_size,\n",
    "            'emd_dim': embedding_size,\n",
    "            'num_heads': num_heads\n",
    "          }, checkpoint_path)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Acc: {acc_train}, Train Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc4b1261-573d-471e-9a6e-2ae8cabb721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gpt_model.state_dict(), 'mha_GPT.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78699580-ae53-4522-be39-27b0e7d2a51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mha_GPT(\n",
       "  (embedding): Embedding(16000, 1024)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (multihead_self_attention): MultiheadSelfAttention(\n",
       "    (linear_queries): ModuleList(\n",
       "      (0-15): 16 x Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (linear_keys): ModuleList(\n",
       "      (0-15): 16 x Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (linear_values): ModuleList(\n",
       "      (0-15): 16 x Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (linear_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (add_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (feed_forward): FeedForward(\n",
       "    (linear1): Linear(in_features=1024, out_features=10, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (linear2): Linear(in_features=10, out_features=1024, bias=True)\n",
       "  )\n",
       "  (add_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=1024, out_features=16000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "trained_gpt_model = Mha_GPT(vocab_size, embedding_size, max_seq_length, num_heads)\n",
    "trained_gpt_model.load_state_dict(torch.load('mha_GPT.pth'))\n",
    "trained_gpt_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68d55470-bf59-4e18-aa0d-5902cfaa694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor([sp.encode_as_ids(\"The apple\")])\n",
    "logits = gpt_model(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f93118f-b4f1-4309-99cf-4bc25abb48f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 16000])\n",
      "tensor([[[1.8244e-06, 3.6753e-08, 7.4458e-04,  ..., 8.0007e-08,\n",
      "          3.3161e-08, 2.6452e-08],\n",
      "         [2.6648e-04, 2.5594e-11, 4.8363e-06,  ..., 1.1386e-10,\n",
      "          1.9330e-11, 1.2572e-11]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([2, 16000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(logits.shape)\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "print(probs)\n",
    "probs = probs[0]\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78a6a891-4d02-4fcd-9055-c92fb566e35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "tensor(0)\n",
      "<unk>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(vocab[1])\n",
    "# Get the index of the word with the highest probability (argmax)\n",
    "max_value, predicted_word_index = torch.max(probs[:, -1], dim=0)\n",
    "print(predicted_word_index)\n",
    "# You may want to convert this index back to the actual word using your vocabulary\n",
    "predicted_word = vocab[predicted_word_index]\n",
    "print(predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ef82424-9e1c-482c-b5eb-de1eeb56af50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tell_me_a_story(text, counter):\n",
    "    counter += 1\n",
    "    token_ids = torch.tensor([sp.encode_as_ids(text)])\n",
    "    \n",
    "    output = trained_gpt_model(token_ids)\n",
    "    \n",
    "    if isinstance(output, tuple) and len(output) == 2:\n",
    "        logits, attention_weights = output\n",
    "    else:\n",
    "        # Handle the case where your model returns only logits\n",
    "        logits = output\n",
    "        attention_weights = None  # You might need to adjust this based on your model's behavior\n",
    "\n",
    "    \n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    predicted_word_index = torch.argmax(probs, dim=-1)\n",
    "   \n",
    "    predicted_word = sp.decode(predicted_word_index[0][-1].item())\n",
    "\n",
    "    # print(predicted_word)\n",
    "   \n",
    "    if  counter == 100:\n",
    "        return text\n",
    "    else:\n",
    "        return tell_me_a_story(text + \" \" + predicted_word, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f9cab418-940e-489b-8ea1-acf523e2deda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In 1983  ex                                                                                                  '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tell_me_a_story(\"In 1983 \", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9c9a7f5-73fc-48a7-a789-a7f8928482a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The red horse  was very excited to play with her . She was very excited . She was very excited to go on her parents . She was so excited to go to go to go to go to go to go to her . She packed her mommy and the kitchen . She was so excited ! He was so excited ! He was so excited ! She was so excited ! He was so excited ! He was so excited ! He was so excited ! He was so excited ! He was so excited ! He was so excited'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tell_me_a_story(\"The red horse \", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56e5440e-002b-4e7b-a809-4a3e27d7bdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Red  was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tell_me_a_story(\"Red \", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad4c8aa-33cd-4341-9d4e-f0a8bf3e8204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_attention_heatmap(attention_weights):\n",
    "    sns.heatmap(attention_weights, cmap=\"viridis\", annot=True, fmt=\".2f\", cbar=False)\n",
    "    plt.show()\n",
    "\n",
    "# Assuming attention_weights is a 2D NumPy array\n",
    "plot_attention_heatmap(attention_weights.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
