{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f440ad9f-7353-4bfe-b09c-1a6e303c2e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "018c20c8-d948-4c22-b7b2-05e991aec89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, max_seq_length):\n",
    "        super(GPT, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(embedding_size, max_seq_length)\n",
    "        \n",
    "        self.self_attention = SelfAttention(embedding_size)\n",
    "       \n",
    "        self.add_norm1 = nn.LayerNorm(embedding_size) \n",
    "        \n",
    "        self.feed_forward = FeedForward(embedding_size, 10)\n",
    "        \n",
    "        self.add_norm = nn.LayerNorm(embedding_size)\n",
    "        \n",
    "        self.fc = nn.Linear(embedding_size, vocab_size)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.embedding(input_ids)\n",
    "        embeddings = self.positional_encoding(embeddings)\n",
    "        attention_output = self.self_attention(embeddings)\n",
    "        output1 = embeddings + self.add_norm1(attention_output)\n",
    "        ff_output = self.feed_forward(output1)\n",
    "        output = output1 + self.add_norm(ff_output)\n",
    "        \n",
    "        # Extract the last word in the sequence\n",
    "        logits = self.fc(output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, input_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        position = torch.arange(0, max_seq_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pos_enc = torch.zeros(1, max_seq_length, d_model)\n",
    "        pos_enc[:, :, 0::2] = torch.sin(position * div_term)\n",
    "        pos_enc[:, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pos_enc', pos_enc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_enc[:, :x.size(1)].detach()\n",
    "        return x\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        # Create linear layers for query, key, and value projections\n",
    "        self.linear_query = nn.Linear(embedding_size, embedding_size)\n",
    "        self.linear_key = nn.Linear(embedding_size, embedding_size)\n",
    "        self.linear_value = nn.Linear(embedding_size, embedding_size)\n",
    "        \n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Project embeddings into query, key, and value spaces\n",
    "        query = self.linear_query(embeddings)\n",
    "        key = self.linear_key(embeddings)\n",
    "        value = self.linear_value(embeddings)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / (query.size(-1) ** 0.5)\n",
    "        \n",
    "        # Apply masking to prevent attending to future positions\n",
    "        seq_length = attention_scores.size(-1)\n",
    "        mask = torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1).bool().to(attention_scores.device)\n",
    "        attention_scores = attention_scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Compute weighted sum (Attention Scores x V)\n",
    "        output = torch.matmul(attention_probs, value)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9808dff5-b97a-43f6-b0f5-05741d22407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "vocab_size = 10000\n",
    "embedding_size = 256\n",
    "max_seq_length = 100  # Set your desired maximum sequence length\n",
    "\n",
    "# Create an instance of SelfAttention\n",
    "self_attention = SelfAttention(embedding_size)\n",
    "\n",
    "# Generate some example token IDs (batch size of 4, sequence length of 10)\n",
    "token_ids = torch.randint(0, vocab_size, (4, 10))\n",
    "\n",
    "# Calculate attention weights and weighted sum\n",
    "#output = self_attention(token_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70017b1e-65ee-423c-986c-a97e8d3bff69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"tinystories_tokeniser.model\")\n",
    "\n",
    "vocab = [sp.id_to_piece(i) for i in range(sp.get_piece_size())]\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 30\n",
    "max_seq_length = 1192\n",
    "list_of_lists = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "tensor_2d = torch.tensor(list_of_lists)\n",
    "gpt_model = GPT(vocab_size, embedding_size, max_seq_length)\n",
    "probs = gpt_model(tensor_2d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0710316-a31c-45b1-b5b8-6d99395a7fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3861, -0.2036,  0.2616,  ...,  0.1122,  0.0513, -0.9487],\n",
      "        [ 0.2843, -0.6717,  1.0048,  ...,  0.0253, -0.3398,  0.2390],\n",
      "        [ 0.9024, -1.8237,  0.8030,  ..., -1.3157, -0.2283,  0.1605]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d21c6f4-69ec-4004-baf7-b02aeedb2600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16000])\n"
     ]
    }
   ],
   "source": [
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bb47d6-dd26-4831-b118-fa3e31c398f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
