{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e894833a-1d83-4ba0-92b6-48ba7f468ff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.9161,  1.1623,  1.9367,  ...,  0.2979,  0.4223, -0.0256],\n",
      "         [-0.5298,  2.0902,  3.2121,  ...,  2.2878,  1.8936, -0.3237],\n",
      "         [ 0.1080,  1.0145,  1.8287,  ..., -0.5290,  0.4808,  1.5674]],\n",
      "\n",
      "        [[ 0.6863,  2.2931,  2.0946,  ...,  2.3419, -0.3191,  0.5993],\n",
      "         [ 0.2239,  1.0939,  0.4497,  ...,  2.3421, -0.3646, -0.0911],\n",
      "         [-0.1492,  1.1713,  1.1080,  ...,  1.1195,  0.6946,  0.3277]],\n",
      "\n",
      "        [[-0.2384,  1.1638,  1.6924,  ...,  0.9439,  1.1253,  0.8946],\n",
      "         [-0.2976,  0.5248,  0.1309,  ...,  1.2506, -0.0638,  0.4851],\n",
      "         [-0.2456,  1.2687,  1.2724,  ...,  2.7020,  2.5096, -1.6218]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([3, 3, 16000])\n",
      "tensor([[[ 0.9464, -1.7262,  1.6538,  ...,  0.0912, -1.9664,  1.3293],\n",
      "         [-0.8220, -2.3519,  0.8809,  ..., -0.8076, -1.6184,  0.3894],\n",
      "         [ 0.0872, -1.7463,  0.0662,  ..., -0.8642, -0.5145, -0.1360]],\n",
      "\n",
      "        [[ 0.2188, -1.2758, -0.1073,  ..., -0.8241, -0.7945,  0.5382],\n",
      "         [-0.0193, -0.7507,  0.5773,  ..., -0.3010,  0.0591, -0.0613],\n",
      "         [ 0.0716, -0.0113, -0.0843,  ..., -0.6206, -1.8156, -0.0064]],\n",
      "\n",
      "        [[-0.3711, -0.8419,  0.6876,  ..., -0.4988, -1.7710,  1.2372],\n",
      "         [ 0.3795, -0.6065,  1.4133,  ...,  0.8675, -0.5058,  1.8742],\n",
      "         [-0.5403,  0.1826,  1.2547,  ..., -0.5379, -1.5345,  0.3612]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pulsar/miniconda3/envs/transformer/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import custom_dataset\n",
    "from custom_dataset import CustomDataset\n",
    "import sentencepiece as spm\n",
    "import my_GPT\n",
    "from mha_GPT import Mha_GPT\n",
    "import pickle\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "import dataset_bes\n",
    "import collate_fn\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "# install for optimiser\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "scaler = GradScaler()\n",
    "# Load the SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"tinystories_tokeniser.model\")\n",
    "\n",
    "vocab = [sp.id_to_piece(i) for i in range(sp.get_piece_size())]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba4b7b1-b6b4-4d35-8402-8b4b87ecf02c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "19ddcf95-b7b8-4a41-9427-5999c8398e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "train_data = data[\"train\"]\n",
    "val_data = data[\"validation\"]\n",
    "ds = dataset_bes.TinyDataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "121e9635-6026-423b-8d13-d2dd525f8943",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "dl = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True, collate_fn=ds.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1472f28-8696-4578-89dc-0b8e1e1930b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 1024\n",
    "max_seq_length = 1194\n",
    "num_heads = 16\n",
    "#batch_size = 64  # You can adjust this based on your specific requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6bdacb84-a686-4073-bfa3-b3f6917afa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = Mha_GPT(vocab_size, embedding_size, max_seq_length, num_heads)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c9519f34-5b25-4c06-87e6-8e4d1f852201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "beta1 = 0.9\n",
    "beta2 = 0.98\n",
    "epsilon = 1e-9\n",
    "learning_rate = 0.001  # Adjust this based on your experimentation\n",
    "\n",
    "# Create Adam optimizer\n",
    "optimizer = Adam(gpt_model.parameters(), lr=learning_rate, betas=(beta1, beta2), eps=epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "72747fbb-44d4-4a5e-b0c0-e62ee6f84baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'checkpoints/'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df750043-d132-47cc-b6fc-6b4e8feab7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbirsenyildiz2018\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/pulsar/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wandb.login(key='0a9583c37867b372d3f0849f8385a8eb0d4c4ec2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e994ae3e-7344-4d8e-9e35-071fdee28036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbirsenyildiz2018\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/pulsar/transformer/wandb/run-20231114_125036-adgqf399</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/birsenyildiz2018/Birsen_GPT/runs/adgqf399' target=\"_blank\">bumbling-salad-92</a></strong> to <a href='https://wandb.ai/birsenyildiz2018/Birsen_GPT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/birsenyildiz2018/Birsen_GPT' target=\"_blank\">https://wandb.ai/birsenyildiz2018/Birsen_GPT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/birsenyildiz2018/Birsen_GPT/runs/adgqf399' target=\"_blank\">https://wandb.ai/birsenyildiz2018/Birsen_GPT/runs/adgqf399</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/birsenyildiz2018/Birsen_GPT/runs/adgqf399?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f8d2083d010>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project ='Birsen_GPT', entity = 'birsenyildiz2018',     \n",
    "    config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"architecture\": \"Multi headed GPT\",\n",
    "    \"dataset\": \"Tiny Stories\",\n",
    "    \"epochs\": 10, \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb0272a-8d4a-4eaf-a445-6ae1b53ae11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "milestones = [1000, 2000, 3000]\n",
    "\n",
    "num_epochs = 10  # Adjust as needed\n",
    "checkpoint_interval = 1\n",
    "for epoch in range(num_epochs):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    step = 1\n",
    "    for batch in dl:\n",
    "       \n",
    "        tokens = batch['input']\n",
    "        true_labels = batch['label']\n",
    "        optimizer.zero_grad()\n",
    "        output = gpt_model(batch['input'])\n",
    "        model_output = output.view(-1, vocab_size)  # Reshape to [batch_size * seq_length, num_classes]\n",
    "        true_labels = true_labels.view(-1)  # Reshape to [batch_size * seq_length]\n",
    "        loss = criterion(model_output, true_labels)\n",
    "        max_indices = torch.argmax(model_output, dim=1)\n",
    "        correct_predictions += ((max_indices - true_labels)==0).sum()\n",
    "        total_predictions += len(true_labels)\n",
    "        # correct_predictions += torch.sum(predictions==label)\n",
    "        # total_predictions += batch_size\n",
    "        acc = correct_predictions/total_predictions\n",
    "         # acc = 0\n",
    "        with autocast():\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if step in milestones:\n",
    "        # Save a checkpoint\n",
    "           checkpoint_path = f'model_checkpoint_step_{step}.pth'\n",
    "           torch.save(gpt_model.state_dict(), checkpoint_path)\n",
    "\n",
    "        step += 1\n",
    "    # Logging and checkpointing\n",
    "        wandb.log({\"acc_train\": acc, \"loss_train\": loss.item(), \"perp_train\": math.exp(loss.item())})\n",
    "        \n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "             checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch + 1}.pt')\n",
    "             torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': gpt_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss_train': loss.item(),\n",
    "            'acc_train': acc,\n",
    "            'perp_train': math.exp(loss.item()),\n",
    "            'batch_size': batch_size,\n",
    "            'emd_dim': embedding_size,\n",
    "            'num_heads': num_heads\n",
    "          }, checkpoint_path)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Acc: {acc_train}, Train Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc4b1261-573d-471e-9a6e-2ae8cabb721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gpt_model.state_dict(), 'mha_GPT.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78699580-ae53-4522-be39-27b0e7d2a51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mha_GPT(\n",
       "  (embedding): Embedding(16000, 512)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (multihead_self_attention): MultiheadSelfAttention(\n",
       "    (linear_queries): ModuleList(\n",
       "      (0-3): 4 x Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (linear_keys): ModuleList(\n",
       "      (0-3): 4 x Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (linear_values): ModuleList(\n",
       "      (0-3): 4 x Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (add_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (feed_forward): FeedForward(\n",
       "    (linear1): Linear(in_features=512, out_features=10, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (linear2): Linear(in_features=10, out_features=512, bias=True)\n",
       "  )\n",
       "  (add_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=512, out_features=16000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "trained_gpt_model = Mha_GPT(vocab_size, embedding_size, max_seq_length, num_heads)\n",
    "trained_gpt_model.load_state_dict(torch.load('mha_GPT.pth'))\n",
    "trained_gpt_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68d55470-bf59-4e18-aa0d-5902cfaa694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = torch.tensor([sp.encode_as_ids(\"The apple\")])\n",
    "logits = gpt_model(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f93118f-b4f1-4309-99cf-4bc25abb48f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 16000])\n",
      "tensor([[[1.8244e-06, 3.6753e-08, 7.4458e-04,  ..., 8.0007e-08,\n",
      "          3.3161e-08, 2.6452e-08],\n",
      "         [2.6648e-04, 2.5594e-11, 4.8363e-06,  ..., 1.1386e-10,\n",
      "          1.9330e-11, 1.2572e-11]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([2, 16000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(logits.shape)\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "print(probs)\n",
    "probs = probs[0]\n",
    "print(probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78a6a891-4d02-4fcd-9055-c92fb566e35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "tensor(0)\n",
      "<unk>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(vocab[1])\n",
    "# Get the index of the word with the highest probability (argmax)\n",
    "max_value, predicted_word_index = torch.max(probs[:, -1], dim=0)\n",
    "print(predicted_word_index)\n",
    "# You may want to convert this index back to the actual word using your vocabulary\n",
    "predicted_word = vocab[predicted_word_index]\n",
    "print(predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ef82424-9e1c-482c-b5eb-de1eeb56af50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tell_me_a_story(text, counter):\n",
    "    counter += 1\n",
    "    token_ids = torch.tensor([sp.encode_as_ids(text)])\n",
    "    \n",
    "    output = trained_gpt_model(token_ids)\n",
    "    \n",
    "    if isinstance(output, tuple) and len(output) == 2:\n",
    "        logits, attention_weights = output\n",
    "    else:\n",
    "        # Handle the case where your model returns only logits\n",
    "        logits = output\n",
    "        attention_weights = None  # You might need to adjust this based on your model's behavior\n",
    "\n",
    "    \n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    predicted_word_index = torch.argmax(probs, dim=-1)\n",
    "   \n",
    "    predicted_word = sp.decode(predicted_word_index[0][-1].item())\n",
    "\n",
    "    # print(predicted_word)\n",
    "   \n",
    "    if  counter == 100:\n",
    "        return text\n",
    "    else:\n",
    "        return tell_me_a_story(text + \" \" + predicted_word, counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f9cab418-940e-489b-8ea1-acf523e2deda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In 1983  ex                                                                                                  '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tell_me_a_story(\"In 1983 \", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a9c9a7f5-73fc-48a7-a789-a7f8928482a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The red horse  and horse and horse and horse and horse and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tell_me_a_story(\"The red horse \", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56e5440e-002b-4e7b-a809-4a3e27d7bdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Red  was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was was'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tell_me_a_story(\"Red \", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad4c8aa-33cd-4341-9d4e-f0a8bf3e8204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_attention_heatmap(attention_weights):\n",
    "    sns.heatmap(attention_weights, cmap=\"viridis\", annot=True, fmt=\".2f\", cbar=False)\n",
    "    plt.show()\n",
    "\n",
    "# Assuming attention_weights is a 2D NumPy array\n",
    "plot_attention_heatmap(attention_weights.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
